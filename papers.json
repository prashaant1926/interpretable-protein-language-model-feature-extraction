[
  {
    "id": "interplm-2025",
    "title": "InterPLM: discovering interpretable features in protein language models via sparse autoencoders",
    "authors": "Elana Simon, James Zou",
    "journal": "Nature Methods",
    "conference": "",
    "year": "2025",
    "url": "https://www.nature.com/articles/s41592-025-02836-7",
    "hypotheses": "H1: PLMs store biological concepts in superposition rather than individual neurons; H2: Sparse autoencoders can extract interpretable biological features; H3: Larger PLMs capture more interpretable concepts",
    "notes": "First systematic framework for extracting interpretable features from protein language models using sparse autoencoders. Identified 2,548 interpretable features per layer from ESM-2, strongly correlating with 143 known biological concepts.",
    "strengths": "Systematic approach, strong biological validation, novel application of SAEs to proteins, automated interpretation pipeline, practical applications demonstrated",
    "weaknesses": "Limited to ESM-2 architecture, computational scalability challenges, validation dependent on existing databases",
    "citation": "Simon, E. & Zou, J. InterPLM: discovering interpretable features in protein language models via sparse autoencoders. Nature Methods (2025)."
  },
  {
    "id": "adams-mechanistic-2025",
    "title": "From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models",
    "authors": "Etowah Adams, Liam Bai, Minji Lee, Yiyang Yu, Mohammed AlQuraishi",
    "journal": "bioRxiv",
    "conference": "ICML 2025 spotlight",
    "year": "2025",
    "url": "https://www.biorxiv.org/content/10.1101/2025.02.06.636901v1",
    "hypotheses": "H1: SAEs can identify biologically interpretable features in pLMs; H2: Features represent both generic and family-specific patterns; H3: Linear probing of SAE features can predict protein properties",
    "notes": "Comprehensive methodology for training and evaluating SAEs on ESM-2. Demonstrates scientific discovery potential through model interpretation. Provides training protocols and evaluation frameworks.",
    "strengths": "Rigorous methodology, comprehensive evaluation framework, scientific discovery applications, community resource development",
    "weaknesses": "Limited architectural diversity, computational resource requirements, validation complexity",
    "citation": "Adams, E. et al. From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models. bioRxiv (2025)."
  },
  {
    "id": "lin-esm2-2023",
    "title": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
    "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
    "journal": "Science",
    "conference": "",
    "year": "2023",
    "url": "https://www.science.org/doi/10.1126/science.ade2574",
    "hypotheses": "H1: Large protein language models can learn atomic-resolution structure from sequence alone; H2: Scaling model parameters improves structural prediction accuracy; H3: End-to-end structure prediction is feasible without MSA",
    "notes": "Landmark paper introducing ESM-2 and ESMFold. Demonstrates that scaling protein language models to 15B parameters enables atomic-resolution structure prediction. Created ESM Metagenomic Atlas with >617 million predicted structures.",
    "strengths": "Breakthrough performance, massive scale application, eliminates MSA requirement, strong empirical validation",
    "weaknesses": "Computational resource intensive, limited interpretability, architecture-specific optimizations",
    "citation": "Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science 379, eade2574 (2023)."
  },
  {
    "id": "silberg-functional-2025",
    "title": "Towards functional annotation with latent protein language model features",
    "authors": "Jake Silberg, Elana Simon, James Zou",
    "journal": "OpenReview",
    "conference": "",
    "year": "2025",
    "url": "https://openreview.net/pdf/b02981cd118c2451bb3aa9cc464dcec11ab5eeac.pdf",
    "hypotheses": "H1: SAE features can identify missing protein annotations; H2: Feature-guided structural alignment improves annotation accuracy; H3: Automated annotation pipelines can scale to metagenomic data",
    "notes": "Develops validation pipeline for protein annotation using SAE features. Identified 491 missing CATH topology annotations and annotated 8,077 metagenomic proteins. Combines database matching, structural alignment, and LLM description.",
    "strengths": "Practical annotation applications, missing annotation discovery, structural consistency validation, metagenomic scale demonstration",
    "weaknesses": "Dependent on existing structural data, validation limited to known annotation systems, computational complexity",
    "citation": "Silberg, J., Simon, E. & Zou, J. Towards functional annotation with latent protein language model features. OpenReview (2025)."
  },
  {
    "id": "nori-knowledge-2023",
    "title": "Identification of Knowledge Neurons in Protein Language Models",
    "authors": "Divya Nori, Shivali Singireddy, Marina Ten Have",
    "journal": "arXiv",
    "conference": "",
    "year": "2023",
    "url": "https://arxiv.org/html/2312.10770v1",
    "hypotheses": "H1: Protein language models contain specialized 'knowledge neurons'; H2: Knowledge neurons are concentrated in attention key vector networks; H3: Activation and gradient-based selection identify functionally relevant neurons",
    "notes": "Applied knowledge neuron identification techniques to ESM models for enzyme classification. Found high density of knowledge neurons in self-attention key vector networks, suggesting specialized understanding of sequence features.",
    "strengths": "Novel application of knowledge neuron concepts, attention mechanism analysis, enzyme classification validation",
    "weaknesses": "Limited to specific task (enzyme classification), small model scale, narrow biological domain",
    "citation": "Nori, D., Singireddy, S. & Ten Have, M. Identification of Knowledge Neurons in Protein Language Models. arXiv:2312.10770 (2023)."
  },
  {
    "id": "zhang-categorical-2024",
    "title": "Categorical Jacobian method for protein language models",
    "authors": "Zhang et al.",
    "journal": "PNAS",
    "conference": "",
    "year": "2024",
    "url": "https://www.pnas.org/doi/10.1073/pnas.2406285121",
    "hypotheses": "H1: Gradient-based methods can reveal functional category processing in PLMs; H2: Categorical jacobians capture biological classification patterns; H3: Alternative to sparse autoencoders for interpretability",
    "notes": "Developed gradient-based interpretability techniques for understanding how PLMs process functional categories. Provides alternative approach to sparse autoencoders for model interpretation.",
    "strengths": "Alternative interpretability method, functional category focus, gradient-based insights",
    "weaknesses": "Limited comparative analysis with SAE methods, narrow scope of biological categories",
    "citation": "Zhang et al. Categorical Jacobian method for protein language models. PNAS (2024)."
  },
  {
    "id": "hyskova-comparison-2025",
    "title": "A Comparison of AlphaFold2, ESMFold, and OmegaFold",
    "authors": "Anna Hýskova, Eva Maršálková, Petr Šimeček",
    "journal": "bioRxiv",
    "conference": "",
    "year": "2025",
    "url": "https://www.biorxiv.org/content/10.1101/2025.06.20.660709v1.full-text",
    "hypotheses": "H1: Different architectures capture different biological features; H2: Architecture choice influences prediction accuracy on specific protein types; H3: Performance can be predicted from sequence and structural features",
    "notes": "Systematic benchmarking of structure prediction methods using 1,327 post-training PDB entries. Demonstrates architecture-specific performance patterns and develops ML classifiers to predict when methods will succeed.",
    "strengths": "Unbiased evaluation dataset, systematic comparison, predictive framework for method selection, practical guidance",
    "weaknesses": "Limited to structure prediction task, post-hoc analysis only, no mechanistic insights",
    "citation": "Hýskova, A., Maršálková, E. & Šimeček, P. A Comparison of AlphaFold2, ESMFold, and OmegaFold. bioRxiv (2025)."
  },
  {
    "id": "van-kempen-clustering-2023",
    "title": "Clustering predicted structures at the scale of the known protein universe",
    "authors": "Milot Mirdita, Martin Steinegger",
    "journal": "Nature",
    "conference": "",
    "year": "2023",
    "url": "https://www.nature.com/articles/s41586-023-06510-w",
    "hypotheses": "H1: Structural clustering reveals unknown protein families; H2: Large-scale analysis identifies evolutionary relationships; H3: Computational methods can scale to full proteome",
    "notes": "Clustered 214 million AlphaFold structures identifying 2.30 million structural clusters. 31% lack annotations representing unknown families. Demonstrates feasibility of proteome-scale structural analysis.",
    "strengths": "Unprecedented scale, novel family discovery, efficient clustering algorithm, evolutionary insights",
    "weaknesses": "Dependent on structure prediction quality, limited functional characterization, computational resource intensive",
    "citation": "van Kempen, M. et al. Clustering predicted structures at the scale of the known protein universe. Nature 622, 637-643 (2023)."
  },
  {
    "id": "zhong-parafold-2022",
    "title": "ParaFold: Paralleling AlphaFold for Large-Scale Predictions",
    "authors": "Bozitao Zhong, Xiaoming Su, Minhua Wen, Sicheng Zuo, Liang Hong, James Lin",
    "journal": "arXiv",
    "conference": "",
    "year": "2022",
    "url": "https://ar5iv.labs.arxiv.org/html/2111.06340",
    "hypotheses": "H1: CPU-GPU separation improves AlphaFold scalability; H2: Multi-threaded parallelism reduces CPU bottlenecks; H3: Optimized compilation improves GPU utilization",
    "notes": "Developed parallel version of AlphaFold enabling large-scale structure predictions. Separates CPU MSA construction from GPU inference to improve utilization and reduce runtime.",
    "strengths": "Practical scalability solution, significant performance improvements, open-source availability",
    "weaknesses": "Architecture-specific optimizations, limited to AlphaFold workflow, infrastructure requirements",
    "citation": "Zhong, B. et al. ParaFold: Paralleling AlphaFold for Large-Scale Predictions. arXiv:2111.06340 (2022)."
  },
  {
    "id": "choudhary-unified-2023",
    "title": "Unified access to up-to-date residue-level annotations from UniProtKB and other biological databases for PDB data",
    "authors": "Preeti Choudhary, Stephen Anyango, John Berrisford",
    "journal": "Scientific Data",
    "conference": "",
    "year": "2023",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10097656/",
    "hypotheses": "H1: Unified database access improves annotation coverage; H2: Cross-referencing reduces annotation gaps; H3: Automated integration maintains data consistency",
    "notes": "Provides unified access to residue-level annotations across UniProtKB, Pfam, SCOP, and PDB. Highlights complexity of cross-database integration and reveals annotation coverage gaps.",
    "strengths": "Comprehensive database integration, systematic annotation mapping, practical resource for community",
    "weaknesses": "Data integration complexity, annotation gaps persist, maintenance overhead",
    "citation": "Choudhary, P., Anyango, S. & Berrisford, J. Unified access to up-to-date residue-level annotations from UniProtKB and other biological databases for PDB data. Sci Data 10, 204 (2023)."
  },
  {
    "id": "blum-interpro-2025",
    "title": "InterPro: the protein sequence classification resource in 2025",
    "authors": "Matthias Blum, Antonina Andreeva, Laise Cavalcanti Florentino, Sara Rocio Chuguransky, Alex Bateman",
    "journal": "Nucleic Acids Research",
    "conference": "",
    "year": "2025",
    "url": "https://discovery.ucl.ac.uk/id/eprint/10200737/",
    "hypotheses": "H1: AI-enhanced descriptions improve protein family characterization; H2: AlphaFold integration enhances structural context; H3: Unified classification improves annotation quality",
    "notes": "Updated InterPro database with AI-enhanced descriptions and increased AlphaFold structure integration. Provides unified protein sequence classification across multiple member databases.",
    "strengths": "AI-enhanced annotations, structural integration, comprehensive classification system",
    "weaknesses": "Dependent on member database quality, annotation lag for new sequences, complexity of unified schema",
    "citation": "Blum, M. et al. InterPro: the protein sequence classification resource in 2025. Nucleic Acids Res (2025)."
  },
  {
    "id": "manfredi-alphafold-2025",
    "title": "AlphaFold2 and ESMFold: A large-scale pairwise model comparison of human enzymes upon Pfam functional annotation",
    "authors": "Matteo Manfredi, Gabriele Vazzana, Castrense Savojardo",
    "journal": "Computational and Structural Biotechnology Journal",
    "conference": "",
    "year": "2025",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11799866/",
    "hypotheses": "H1: AlphaFold2 and ESMFold show complementary strengths; H2: Functional annotation influences performance comparison; H3: Architectural differences impact specific enzyme classes",
    "notes": "Large-scale pairwise comparison of AlphaFold2 and ESMFold on human enzymes with Pfam annotation. Reveals architecture-specific performance patterns across different functional categories.",
    "strengths": "Large-scale systematic comparison, functional annotation focus, enzyme-specific analysis",
    "weaknesses": "Limited to human enzymes, comparison metrics focused on structure, narrow functional scope",
    "citation": "Manfredi, M., Vazzana, G. & Savojardo, C. AlphaFold2 and ESMFold: A large-scale pairwise model comparison of human enzymes upon Pfam functional annotation. Comput Struct Biotechnol J 27, 461-466 (2025)."
  },
  {
    "id": "saadat-missense-2025",
    "title": "Fine-tuning protein language models to understand the functional impact of missense variants",
    "authors": "Ali Saadat, Jacques Fellay",
    "journal": "Computational and Structural Biotechnology Journal",
    "conference": "",
    "year": "2025",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12166733/",
    "hypotheses": "H1: Fine-tuning improves variant effect prediction; H2: PLMs capture mutational constraints; H3: Transfer learning applies to clinical genetics",
    "notes": "Demonstrates fine-tuning of protein language models for predicting functional impact of missense variants. Shows transfer learning applicability to clinical genetics applications.",
    "strengths": "Clinical application focus, variant effect prediction, transfer learning methodology",
    "weaknesses": "Limited to missense variants, validation challenges for clinical data, interpretability not addressed",
    "citation": "Saadat, A. & Fellay, J. Fine-tuning protein language models to understand the functional impact of missense variants. Comput Struct Biotechnol J 27, 2199-2207 (2025)."
  },
  {
    "id": "reticular-structure-2025",
    "title": "Towards Interpretable Protein Structure Prediction with Sparse Autoencoders",
    "authors": "Reticular AI Research Team",
    "journal": "Reticular AI Research",
    "conference": "",
    "year": "2025",
    "url": "https://www.reticular.ai/research/interpretable-protein-structure-prediction",
    "hypotheses": "H1: SAEs can be scaled to ESM2-3B for structure prediction; H2: Matryoshka SAEs provide hierarchical feature organization; H3: Interpretable features explain structure prediction mechanisms",
    "notes": "First application of sparse autoencoders to ESM2-3B for mechanistic understanding of ESMFold structure prediction. Introduces Matryoshka SAEs with hierarchically organized features.",
    "strengths": "Large model scale, structure prediction focus, hierarchical feature organization, mechanistic insights",
    "weaknesses": "Industry research with limited peer review, specific to ESMFold architecture, validation scope unclear",
    "citation": "Reticular AI Research Team. Towards Interpretable Protein Structure Prediction with Sparse Autoencoders. Reticular AI Research (2025)."
  },
  {
    "id": "bordin-clustering-2024",
    "title": "Clustering protein functional families at large scale with hierarchical approaches",
    "authors": "Nicola Bordin, Harry Scholes, Clemens Rauer",
    "journal": "Protein Science",
    "conference": "",
    "year": "2024",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11325189/",
    "hypotheses": "H1: Hierarchical clustering reveals functional relationships; H2: Large-scale analysis identifies novel families; H3: Computational methods scale to full proteomes",
    "notes": "Develops hierarchical approaches for clustering protein functional families at large scale. Demonstrates scalability to proteome-level analysis while maintaining biological relevance.",
    "strengths": "Hierarchical methodology, large-scale application, functional family focus",
    "weaknesses": "Limited interpretability analysis, computational resource requirements, validation complexity",
    "citation": "Bordin, N., Scholes, H. & Rauer, C. Clustering protein functional families at large scale with hierarchical approaches. Protein Sci 33, e5140 (2024)."
  },
  {
    "id": "wilke-transfer-2025",
    "title": "Medium-sized protein language models perform well at transfer learning tasks",
    "authors": "Claus O. Wilke",
    "journal": "Scientific Reports",
    "conference": "",
    "year": "2025",
    "url": "https://www.nature.com/articles/s41598-025-05674-x",
    "hypotheses": "H1: Medium-sized PLMs are sufficient for many tasks; H2: Transfer learning effectiveness depends on model architecture; H3: Computational efficiency can be maintained without performance loss",
    "notes": "Systematic evaluation of medium-sized protein language models for transfer learning tasks. Shows that computational efficiency can be achieved without significant performance degradation.",
    "strengths": "Efficiency focus, systematic evaluation, practical guidance for resource-limited applications",
    "weaknesses": "Limited to medium-sized models, narrow task scope, no interpretability analysis",
    "citation": "Wilke, C.O. Medium-sized protein language models perform well at transfer learning tasks. Sci Rep 15, 5674 (2025)."
  },
  {
    "id": "abramson-alphafold3-2024",
    "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
    "authors": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel",
    "journal": "Nature",
    "conference": "",
    "year": "2024",
    "url": "https://www.nature.com/articles/s41586-024-07487-w_reference.pdf",
    "hypotheses": "H1: Diffusion models improve biomolecular interaction prediction; H2: Multi-modal training enhances accuracy; H3: Confidence estimation enables practical applications",
    "notes": "Introduces AlphaFold 3 with diffusion-based architecture for predicting biomolecular interactions. Represents significant advance in multi-molecular structure prediction.",
    "strengths": "Major architectural innovation, interaction prediction capability, broad biomolecular scope",
    "weaknesses": "Limited interpretability, high computational requirements, proprietary aspects limit access",
    "citation": "Abramson, J. et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature (2024)."
  },
  {
    "id": "gong-spired-2024",
    "title": "An end-to-end framework for the prediction of protein structure and fitness from single sequence",
    "authors": "Haipeng Gong",
    "journal": "Nature Communications",
    "conference": "",
    "year": "2024",
    "url": "https://www.nature.com/articles/s41467-024-51776-x",
    "hypotheses": "H1: Single-sequence methods can achieve competitive accuracy; H2: Computational efficiency enables large-scale applications; H3: Fitness prediction integrates with structure prediction",
    "notes": "Introduces SPIRED framework for accelerated structure and fitness prediction from single sequences. Achieves 5-fold speed improvement while maintaining accuracy.",
    "strengths": "Significant speed improvements, integrated structure-fitness prediction, single-sequence approach",
    "weaknesses": "Limited to single sequences, validation scope unclear, interpretability not addressed",
    "citation": "Gong, H. An end-to-end framework for the prediction of protein structure and fitness from single sequence. Nat Commun 15, 51776 (2024)."
  },
  {
    "id": "elnaggar-protrans-2021",
    "title": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing",
    "authors": "Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost",
    "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "conference": "",
    "year": "2021",
    "url": "https://ieeexplore.ieee.org/document/9477085",
    "hypotheses": "H1: BERT architectures apply effectively to protein sequences; H2: Self-supervised learning captures biological patterns; H3: Large-scale training improves downstream task performance",
    "notes": "Pioneering work applying BERT architectures to protein sequences. Established the ProtTrans family including ProtBERT, demonstrating that masked language modeling captures biologically relevant patterns.",
    "strengths": "Foundational contribution, systematic architecture exploration, comprehensive downstream evaluation",
    "weaknesses": "Limited interpretability analysis, computational resource intensive, early methodology",
    "citation": "Elnaggar, A. et al. ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing. IEEE Trans Pattern Anal Mach Intell 44, 7112-7127 (2021)."
  },
  {
    "id": "ferrucci-hadoop-2018",
    "title": "Using Hadoop MapReduce for parallel genetic algorithms: A comparison of the global, grid and island models",
    "authors": "F. Ferrucci, P. Salza, F. Sarro",
    "journal": "Evolutionary Computation",
    "conference": "",
    "year": "2018",
    "url": "https://discovery.ucl.ac.uk/id/eprint/10065139/",
    "hypotheses": "H1: Distributed computing improves genetic algorithm scalability; H2: Different parallelization strategies have distinct performance characteristics; H3: Hadoop MapReduce enables large-scale evolutionary computation",
    "notes": "Comprehensive comparison of parallel genetic algorithm models using Hadoop MapReduce. Provides insights relevant to distributed approaches for large-scale protein analysis.",
    "strengths": "Systematic parallelization comparison, practical distributed computing insights, comprehensive evaluation",
    "weaknesses": "Not protein-specific, older computational paradigm, limited to genetic algorithms",
    "citation": "Ferrucci, F., Salza, P. & Sarro, F. Using Hadoop MapReduce for parallel genetic algorithms: A comparison of the global, grid and island models. Evol Comput 26, 535-567 (2018)."
  }
]